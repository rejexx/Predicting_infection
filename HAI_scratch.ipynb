{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "regulated-alert",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas_profiling import ProfileReport\n",
    "import plotly.express as px\n",
    "\n",
    "from sklearn import __version__ as sklearn_version\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV, learning_curve\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fifteen-noise",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3096, 77)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import data\n",
    "HAI_tidy = pd.read_csv('.\\data\\HAI_tidy_Wrangled.csv')\n",
    "HAI_tidy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "hourly-citizen",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1715, 77)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Drop anything with NA in the predictor col\n",
    "HAI_tidy.dropna(subset=['HAI_5_SIR_Score'], inplace=True)\n",
    "HAI_tidy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "exciting-league",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1715 entries, 0 to 3095\n",
      "Data columns (total 75 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   Facility ID                   1715 non-null   object \n",
      " 1   HAI_1_CILOWER                 1484 non-null   float64\n",
      " 2   HAI_1_CIUPPER                 1675 non-null   float64\n",
      " 3   HAI_1_DOPC                    1712 non-null   float64\n",
      " 4   HAI_1_ELIGCASES               1712 non-null   float64\n",
      " 5   HAI_1_NUMERATOR               1712 non-null   float64\n",
      " 6   HAI_1_SIR_Score               1675 non-null   float64\n",
      " 7   HAI_2_CILOWER                 1568 non-null   float64\n",
      " 8   HAI_2_CIUPPER                 1689 non-null   float64\n",
      " 9   HAI_2_DOPC                    1712 non-null   float64\n",
      " 10  HAI_2_ELIGCASES               1712 non-null   float64\n",
      " 11  HAI_2_NUMERATOR               1712 non-null   float64\n",
      " 12  HAI_2_SIR_Score               1689 non-null   float64\n",
      " 13  HAI_3_CILOWER                 1304 non-null   float64\n",
      " 14  HAI_3_CIUPPER                 1551 non-null   float64\n",
      " 15  HAI_3_DOPC                    1704 non-null   float64\n",
      " 16  HAI_3_ELIGCASES               1704 non-null   float64\n",
      " 17  HAI_3_NUMERATOR               1704 non-null   float64\n",
      " 18  HAI_3_SIR_Score               1551 non-null   float64\n",
      " 19  HAI_4_CILOWER                 603 non-null    float64\n",
      " 20  HAI_4_CIUPPER                 780 non-null    float64\n",
      " 21  HAI_4_DOPC                    1665 non-null   float64\n",
      " 22  HAI_4_ELIGCASES               1665 non-null   float64\n",
      " 23  HAI_4_NUMERATOR               1665 non-null   float64\n",
      " 24  HAI_4_SIR_Score               780 non-null    float64\n",
      " 25  HAI_5_CILOWER                 1454 non-null   float64\n",
      " 26  HAI_5_CIUPPER                 1715 non-null   float64\n",
      " 27  HAI_5_DOPC                    1715 non-null   float64\n",
      " 28  HAI_5_ELIGCASES               1715 non-null   float64\n",
      " 29  HAI_5_NUMERATOR               1715 non-null   float64\n",
      " 30  HAI_5_SIR_Score               1715 non-null   float64\n",
      " 31  HAI_6_CILOWER                 1702 non-null   float64\n",
      " 32  HAI_6_CIUPPER                 1715 non-null   float64\n",
      " 33  HAI_6_DOPC                    1715 non-null   float64\n",
      " 34  HAI_6_ELIGCASES               1715 non-null   float64\n",
      " 35  HAI_6_NUMERATOR               1715 non-null   float64\n",
      " 36  HAI_6_SIR_Score               1715 non-null   float64\n",
      " 37  HAI_1_SIR_ComparedToNational  1675 non-null   object \n",
      " 38  HAI_2_SIR_ComparedToNational  1689 non-null   object \n",
      " 39  HAI_3_SIR_ComparedToNational  1551 non-null   object \n",
      " 40  HAI_4_SIR_ComparedToNational  780 non-null    object \n",
      " 41  HAI_5_SIR_ComparedToNational  1715 non-null   object \n",
      " 42  HAI_6_SIR_ComparedToNational  1715 non-null   object \n",
      " 43  Facility Name                 1715 non-null   object \n",
      " 44  Address                       1715 non-null   object \n",
      " 45  City                          1715 non-null   object \n",
      " 46  State                         1715 non-null   object \n",
      " 47  ZIP Code                      1715 non-null   int64  \n",
      " 48  County Name                   1715 non-null   object \n",
      " 49  H_CLEAN_STAR_RATING           1693 non-null   float64\n",
      " 50  H_COMP_1_STAR_RATING          1693 non-null   float64\n",
      " 51  H_COMP_2_STAR_RATING          1693 non-null   float64\n",
      " 52  H_COMP_3_STAR_RATING          1693 non-null   float64\n",
      " 53  H_COMP_5_STAR_RATING          1693 non-null   float64\n",
      " 54  H_COMP_6_STAR_RATING          1693 non-null   float64\n",
      " 55  H_COMP_7_STAR_RATING          1693 non-null   float64\n",
      " 56  H_HSP_RATING_STAR_RATING      1693 non-null   float64\n",
      " 57  H_QUIET_STAR_RATING           1693 non-null   float64\n",
      " 58  H_RECMND_STAR_RATING          1693 non-null   float64\n",
      " 59  H_STAR_RATING                 1693 non-null   float64\n",
      " 60  H_CLEAN_LINEAR_SCORE          1693 non-null   float64\n",
      " 61  H_COMP_1_LINEAR_SCORE         1693 non-null   float64\n",
      " 62  H_COMP_2_LINEAR_SCORE         1693 non-null   float64\n",
      " 63  H_COMP_3_LINEAR_SCORE         1693 non-null   float64\n",
      " 64  H_COMP_5_LINEAR_SCORE         1693 non-null   float64\n",
      " 65  H_COMP_6_LINEAR_SCORE         1693 non-null   float64\n",
      " 66  H_COMP_7_LINEAR_SCORE         1693 non-null   float64\n",
      " 67  H_HSP_RATING_LINEAR_SCORE     1693 non-null   float64\n",
      " 68  H_QUIET_LINEAR_SCORE          1693 non-null   float64\n",
      " 69  H_RECMND_LINEAR_SCORE         1693 non-null   float64\n",
      " 70  SEP_1                         1633 non-null   float64\n",
      " 71  SEP_SH_3HR                    1629 non-null   float64\n",
      " 72  SEP_SH_6HR                    1291 non-null   float64\n",
      " 73  SEV_SEP_3HR                   1633 non-null   float64\n",
      " 74  SEV_SEP_6HR                   1626 non-null   float64\n",
      "dtypes: float64(62), int64(1), object(12)\n",
      "memory usage: 1018.3+ KB\n"
     ]
    }
   ],
   "source": [
    "#Drop info cols\n",
    "HAI_tidy.drop(['Phone Number', 'Location'], axis=\"columns\", inplace=True)\n",
    "HAI_tidy.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "conceptual-penalty",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop upper and lower limts from HAI data\n",
    "HAI_tidy = HAI_tidy[HAI_tidy.columns.drop(HAI_tidy.filter(regex='CILOWER|CIUPPER|DOPC|ELIG|NUMERATOR|STAR').columns )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "generic-clinic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['HAI_1_SIR_Score', 'HAI_2_SIR_Score', 'HAI_3_SIR_Score',\n",
       "       'HAI_4_SIR_Score', 'HAI_5_SIR_Score', 'HAI_6_SIR_Score', 'ZIP Code',\n",
       "       'H_CLEAN_LINEAR_SCORE', 'H_COMP_1_LINEAR_SCORE',\n",
       "       'H_COMP_2_LINEAR_SCORE', 'H_COMP_3_LINEAR_SCORE',\n",
       "       'H_COMP_5_LINEAR_SCORE', 'H_COMP_6_LINEAR_SCORE',\n",
       "       'H_COMP_7_LINEAR_SCORE', 'H_HSP_RATING_LINEAR_SCORE',\n",
       "       'H_QUIET_LINEAR_SCORE', 'H_RECMND_LINEAR_SCORE', 'SEP_1', 'SEP_SH_3HR',\n",
       "       'SEP_SH_6HR', 'SEV_SEP_3HR', 'SEV_SEP_6HR'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "\n",
    "HAI_tidy = HAI_tidy.select_dtypes(include=numerics)\n",
    "HAI_tidy.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "composite-acoustic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['HAI_1_SIR_Score', 'HAI_2_SIR_Score', 'HAI_3_SIR_Score',\n",
      "       'HAI_4_SIR_Score', 'HAI_6_SIR_Score', 'ZIP Code',\n",
      "       'H_CLEAN_LINEAR_SCORE', 'H_COMP_1_LINEAR_SCORE',\n",
      "       'H_COMP_2_LINEAR_SCORE', 'H_COMP_3_LINEAR_SCORE',\n",
      "       'H_COMP_5_LINEAR_SCORE', 'H_COMP_6_LINEAR_SCORE',\n",
      "       'H_COMP_7_LINEAR_SCORE', 'H_HSP_RATING_LINEAR_SCORE',\n",
      "       'H_QUIET_LINEAR_SCORE', 'H_RECMND_LINEAR_SCORE', 'SEP_1', 'SEP_SH_3HR',\n",
      "       'SEP_SH_6HR', 'SEV_SEP_3HR', 'SEV_SEP_6HR'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(HAI_tidy.drop(columns='HAI_5_SIR_Score'), \n",
    "                                                    HAI_tidy.HAI_5_SIR_Score, test_size=0.3, \n",
    "                                                    random_state=47)\n",
    "print(X_train.columns) #removed target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "wooden-reverse",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8090583333333325"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Code task 4#\n",
    "#Calculate the mean of `y_train`\n",
    "train_mean = y_train.mean()\n",
    "train_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "according-theta",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit the dummy regressor on the training data\n",
    "dumb_reg = DummyRegressor(strategy='mean')\n",
    "dumb_reg.fit(X_train, y_train)\n",
    "y_tr_pred = dumb_reg.predict(X_train)\n",
    "y_te_pred = train_mean * np.ones(len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "north-prototype",
   "metadata": {},
   "source": [
    "Check the \"mean\" model metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "heard-production",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, -0.0010541157440056015)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_train, y_tr_pred), r2_score(y_test, y_te_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "automated-picking",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.49731080555555557, 0.45229762135922325)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_train, y_tr_pred), mean_absolute_error(y_test, y_te_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "wicked-throw",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4560547265972222, 0.38023078641248653)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_train, y_tr_pred), mean_squared_error(y_test, y_te_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aquatic-oklahoma",
   "metadata": {},
   "source": [
    "Simplistic regression with imputation, taken from the guided capstone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baking-noise",
   "metadata": {},
   "source": [
    "##### These are the values we'll use to fill in any missing values.  \n",
    "X_defaults_median = X_train.median()\n",
    "X_defaults_median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dramatic-event",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_defaults_median' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-0030e1e72968>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Call `X_train` and `X_test`'s `fillna()` method, passing `X_defaults_median` as the values to use\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#Assign the results to `X_tr` and `X_te`, respectively\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mX_tr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_defaults_median\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mX_te\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_defaults_median\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_defaults_median' is not defined"
     ]
    }
   ],
   "source": [
    "#Call `X_train` and `X_test`'s `fillna()` method, passing `X_defaults_median` as the values to use\n",
    "#Assign the results to `X_tr` and `X_te`, respectively\n",
    "X_tr = X_train.fillna(X_defaults_median)\n",
    "X_te = X_test.fillna(X_defaults_median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consistent-spread",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call the StandardScaler`s fit method on `X_tr` to fit the scaler\n",
    "#then use it's `transform()` method to apply the scaling to both the train and test split\n",
    "#data (`X_tr` and `X_te`), naming the results `X_tr_scaled` and `X_te_scaled`, respectively\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_tr)\n",
    "X_tr_scaled = scaler.transform(X_tr)\n",
    "X_te_scaled = scaler.transform(X_te)\n",
    "X_tr_scaled[:5,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continued-strain",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LinearRegression().fit(X_tr_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "union-employment",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code task 11#\n",
    "#Call the `predict()` method of the model (`lm`) on both the (scaled) train and test data\n",
    "#Assign the predictions to `y_tr_pred` and `y_te_pred`, respectively\n",
    "y_tr_pred = lm.predict(X_tr_scaled)\n",
    "y_te_pred = lm.predict(X_te_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demographic-florist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# r^2 - train, test\n",
    "median_r2 = r2_score(y_train, y_tr_pred), r2_score(y_test, y_te_pred)\n",
    "median_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stunning-stomach",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code task 12#\n",
    "#Now calculate the mean absolute error scores using `sklearn`'s `mean_absolute_error` function\n",
    "# as we did above for R^2\n",
    "# MAE - train, test\n",
    "median_mae = mean_absolute_error(y_train, y_tr_pred), mean_absolute_error(y_test, y_te_pred)\n",
    "median_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historic-packing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code task 13#\n",
    "#And also do the same using `sklearn`'s `mean_squared_error`\n",
    "# MSE - train, test\n",
    "median_mse = mean_squared_error(y_train, y_tr_pred), mean_squared_error(y_test, y_te_pred)\n",
    "np.sqrt(median_mse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
